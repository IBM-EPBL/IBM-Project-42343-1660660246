{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "adcf7646",
   "metadata": {},
   "source": [
    "# Sprint 2\n",
    "\n",
    "## Team ID: PNT2022TMID43387\n",
    "\n",
    "###  Importing the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cfd173cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow #both ML and DL for computation\n",
    "from tensorflow.keras.datasets import mnist #mnist dataset\n",
    "from tensorflow.keras.models import Sequential #plain stack of layers\n",
    "from tensorflow.keras import layers #A Layer consists of a tensor- in tensor-out computat ion funct ion\n",
    "from tensorflow.keras.layers import Dense, Flatten #dense and flatten layers\n",
    "from tensorflow.keras.layers import Conv2D #onvoLutiona l Layer\n",
    "from keras.optimizers import Adam #optimizer\n",
    "from keras. utils import np_utils #used for one-hot encoding\n",
    "import matplotlib.pyplot as plt   #used for data visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb53b2f",
   "metadata": {},
   "source": [
    "### Data preprocessing - Sprint 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a57e9649",
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train,y_train),(x_test,y_test)=mnist.load_data()\n",
    "#CNN expected format: (batch,height,width,channel)\n",
    "x_train=x_train.reshape(60000,28,28,1).astype('float32')\n",
    "x_test=x_test.reshape(10000,28,28,1).astype('float32')\n",
    "no_of_classes=10\n",
    "y_train=np_utils.to_categorical(y_train,no_of_classes) #converts output to binary format\n",
    "y_test=np_utils.to_categorical(y_test,no_of_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83cddd7a",
   "metadata": {},
   "source": [
    "### Add CNN Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "37583249",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create model\n",
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6271510b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#add model layer\n",
    "model.add(Conv2D(64, (3, 3), input_shape=(28, 28, 1), activation='relu'))\n",
    "model.add(Conv2D(32, (3, 3), activation = 'relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0da426ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#flatten the dimension of the image\n",
    "model.add(Flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "134d093c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#output layer with 10 neurons\n",
    "model.add(Dense(no_of_classes,activation = 'softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e259ec",
   "metadata": {},
   "source": [
    "### Compiling the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b90717fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compile model\n",
    "model.compile(loss= 'categorical_crossentropy', optimizer=\"Adam\", metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d99396d",
   "metadata": {},
   "source": [
    "Compilation requires 3 arguments: an optimizer, a loss function, and a list of metrics.\n",
    "In our project, we have 2 classes in the output, so the loss is binary_crossentropy.\n",
    "If you have more than two classes in output put “loss = categorical_cross entropy”. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bce4ba8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.asarray(x_train)\n",
    "y_train = np.asarray(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a0c3af3",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5721e18d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1875/1875 [==============================] - 129s 67ms/step - loss: 0.2592 - accuracy: 0.9495 - val_loss: 0.0900 - val_accuracy: 0.9727\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 126s 67ms/step - loss: 0.0702 - accuracy: 0.9786 - val_loss: 0.0878 - val_accuracy: 0.9738\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 126s 67ms/step - loss: 0.0499 - accuracy: 0.9840 - val_loss: 0.1123 - val_accuracy: 0.9707\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 126s 67ms/step - loss: 0.0363 - accuracy: 0.9883 - val_loss: 0.0959 - val_accuracy: 0.9759\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 128s 68ms/step - loss: 0.0280 - accuracy: 0.9914 - val_loss: 0.0988 - val_accuracy: 0.9746\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2283b43d7f0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#fit the model\n",
    "model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=5, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a194212a",
   "metadata": {},
   "source": [
    "Arguments:\n",
    "steps_per_epoch : it specifies the total number of steps taken from the generator as soon as one epoch is finished and the next epoch has started. We can calculate the value of steps_per_epoch as the total number of samples in your dataset divided by the batch size.\n",
    "\n",
    "Epochs: an integer and number of epochs we want to train our model for.\n",
    "\n",
    "Validation_data :  \n",
    "an inputs and targets list\n",
    "a generator\n",
    "inputs, targets, and sample_weights list which can be used to evaluate the loss and metrics for any model after any epoch has ended.\n",
    "\n",
    "\n",
    "validation_steps: only if the validation_data is a generator then only this argument can be used. It specifies the total number of steps taken from the generator before it is stopped at every epoch and its value is calculated as the total number of validation data points in your dataset divided by the validation batch size."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c8d7ad6",
   "metadata": {},
   "source": [
    "### Observing the metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d1e02db1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics(Test loss & Test Accuracy) : \n",
      "[0.09875915944576263, 0.9746000170707703]\n"
     ]
    }
   ],
   "source": [
    "#final evaluation of the model\n",
    "metrics = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"Metrics(Test loss & Test Accuracy) : \")\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85de749c",
   "metadata": {},
   "source": [
    "### Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1fe2d08b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 50ms/step\n",
      "[[9.29056476e-09 1.66384058e-15 2.57575544e-10 3.74265561e-08\n",
      "  2.68239641e-13 9.65065701e-13 2.44488714e-19 1.00000000e+00\n",
      "  4.10227674e-09 4.70653418e-08]\n",
      " [2.75709316e-12 1.35219971e-12 1.00000000e+00 3.30115061e-15\n",
      "  1.83975963e-14 1.80805668e-19 1.52425489e-10 1.13207219e-12\n",
      "  1.14185896e-12 1.32066102e-17]\n",
      " [1.24434383e-08 9.99994040e-01 2.72169723e-06 2.40772194e-11\n",
      "  2.62443655e-06 3.34774285e-07 1.79220727e-08 5.60832554e-08\n",
      "  2.22013782e-07 1.91375152e-10]\n",
      " [9.99998927e-01 4.73297746e-12 4.94200890e-07 1.72818866e-12\n",
      "  1.03699133e-13 2.64843814e-10 9.34312396e-08 2.16153637e-08\n",
      "  4.97542807e-09 4.37885234e-07]]\n"
     ]
    }
   ],
   "source": [
    "prediction=model.predict(x_test[:4])\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7daa61e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7 2 1 0]\n",
      "[[0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(np.argmax(prediction,axis=1)) #printing labels from first 4 images\n",
    "print(y_test[:4]) #printing the actual labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9983d615",
   "metadata": {},
   "source": [
    "As we already predicted the input from the x_test. According to that by using argmax function here we are printing the labels with high prediction values "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b5dcdb8",
   "metadata": {},
   "source": [
    "### Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "406bdb6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the model\n",
    "model.save('models/mnistCNN.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0beff511",
   "metadata": {},
   "source": [
    "The model is saved with .h5 extension as follows:\n",
    "An H5 file is a data file saved in the Hierarchical Data Format (HDF). It contains multidimensional arrays of scientific data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
